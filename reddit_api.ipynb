{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd483961",
   "metadata": {},
   "source": [
    "### REDDIT API Connexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cafe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from secret import CLIENT_ID, SECRET, USER, PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5b4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c865fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'grant_type' : 'password', \n",
    "    'username' : USER, # a modifier\n",
    "    'password' : PASSWORD # a modifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e41f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "res  = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers = headers)\n",
    "TOKEN = res.json()['access_token']\n",
    "headers = {**headers, **{'Authorization': f'bearer {TOKEN}'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0548cab",
   "metadata": {},
   "source": [
    "## Récupération des comptes d'un SubReddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f783181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'limit': 15000,  # Limite le nombre de résultats à 100 par page\n",
    "    'after': None  # Permet de récupérer les résultats suivants\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recupAccounts(subName: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère les auteurs des posts d'un subreddit\n",
    "    \n",
    "    :param subName: nom du subreddit\n",
    "    :return: DataFrame contenant les auteurs des posts\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    df = pd.DataFrame()\n",
    "    url = f'https://oauth.reddit.com/r/{subName}/hot'\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        i +=1\n",
    "        print(i)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for post in data['data']['children']:\n",
    "                new_row = pd.DataFrame({'author': [post['data']['author']]})\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "            \n",
    "            params['after'] = data['data']['after']\n",
    "\n",
    "            # Arrêtez la boucle si 'after' est None, ce qui signifie qu'il n'y a plus de résultats à récupérer\n",
    "            if params['after'] is None:\n",
    "                break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopSubReddit = pd.read_csv('C:/Users/samma/Desktop/ultimatelist.csv') # CSV des SubReddit les plus populaires "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0365bf",
   "metadata": {},
   "source": [
    "### Récupération des SubReddit d'un compte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6af980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupSubReddit(username: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère les subreddits où un utilisateur a posté.\n",
    "    Cette fonction prend en entrée le nom d'utilisateur d'un utilisateur Reddit et renvoie un DataFrame contenant les subreddits où cet utilisateur a posté.\n",
    "    \n",
    "    :param username: Nom de l'utilisateur Reddit.\n",
    "    :type username: str\n",
    "    :return: DataFrame contenant les subreddits où l'utilisateur a posté.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    dfSub = pd.DataFrame()\n",
    "    after = None\n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "    # Récupère les subreddits où l'utilisateur a posté\n",
    "    while True:\n",
    "        url = f'https://www.reddit.com/user/{username}/submitted.json'\n",
    "        params = {'after': after} if after else {}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        # Si la requête est réussie, ajoutez les subreddits à dfSub\n",
    "        if response.status_code == 200:\n",
    "            submissions = response.json()['data']['children']\n",
    "            for submission in submissions:\n",
    "                new_row = pd.DataFrame({username: [submission['data']['subreddit']]})\n",
    "                dfSub = pd.concat([dfSub, new_row], ignore_index=True)\n",
    "\n",
    "            after = response.json()['data']['after']\n",
    "            if not after:\n",
    "                break\n",
    "        \n",
    "        # Si l'utilisateur n'existe pas, imprimez un message et arrêtez la boucle\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"Utilisateur {username} non trouvé.\")\n",
    "            break\n",
    "\n",
    "        # Si le code d'état est 429, attendez 20 secondes avant de réessayer\n",
    "        elif response.status_code == 429:\n",
    "            wait_time = 20  # Ajoute un délai d'attente de 20 sec pour la réinitialisation\n",
    "            print(f\"Erreur 429 - Trop de requêtes. Attendez {wait_time} secondes avant de réessayer.\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        else:\n",
    "            print(f\"La requête a échoué avec le code d'état {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return dfSub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupAutoSub(accounts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère les subreddits où une liste d'utilisateurs a posté.\n",
    "    \n",
    "    :param accounts: DataFrame contenant les auteurs des posts\n",
    "    :return: DataFrame contenant les subreddits où les utilisateurs ont posté\n",
    "    \"\"\"\n",
    "    dfs = []  # Liste pour stocker les DataFrames des subreddits de chaque utilisateur\n",
    "    i = 0  # Compteur pour suivre le nombre d'utilisateurs traités\n",
    "\n",
    "    for compte in accounts.itertuples(index=False):\n",
    "        print(i)\n",
    "        i += 1\n",
    "        username = compte.author\n",
    "\n",
    "        # Vérifie que le nom d'utilisateur n'est pas '[deleted]' et n'est pas vide\n",
    "        if username != '[deleted]' and username:\n",
    "            # Récupère les subreddits de l'utilisateur et supprime les doublons\n",
    "            df_sub = recupSubReddit(username).drop_duplicates()\n",
    "            dfs.append(df_sub)\n",
    "\n",
    "    # Concatène tous les DataFrames des subreddits en un seul DataFrame\n",
    "    df_result = pd.concat(dfs, axis=1)\n",
    "    return df_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction prinipale \n",
    "\n",
    "def mainFonct(nbSub: int):\n",
    "    \"\"\"\n",
    "    Fonction principale pour récupérer les subreddits où les utilisateurs ont posté.\n",
    "    Args:\n",
    "        nbSub (int): The number of subreddits to process.\n",
    "    \"\"\"\n",
    "    for i in range(0, nbSub):\n",
    "        print(i)\n",
    "\n",
    "        # Remplir le nom du SubReddit de départ\n",
    "        sub_name = 'france'\n",
    "        print(sub_name)\n",
    "        \n",
    "        # Appel de la fonction recupAccounts\n",
    "        df_accounts = recupAccounts(sub_name)\n",
    "        \n",
    "        # Affichage du DataFrame\n",
    "        print(df_accounts)\n",
    "        \n",
    "        # Affichage de la forme du DataFrame\n",
    "        print(df_accounts.shape)\n",
    "\n",
    "        maintenant = datetime.now()\n",
    "        date_heure_lisible = maintenant.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "        print(f\"\\n\\n\\nDébut de {sub_name}\\n {date_heure_lisible}\\n\\n\\n\")\n",
    "        \n",
    "        df_result = recupAutoSub(df_accounts)\n",
    "        df_result.to_csv(f'CSV_SubReddit/{TopSubReddit[\"announcements\"][i]}', index=False)\n",
    "\n",
    "        maintenant = datetime.now()\n",
    "        date_heure_lisible = maintenant.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "        print(f\"\\n\\n\\Fin de {sub_name}\\n {date_heure_lisible}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9c945",
   "metadata": {},
   "source": [
    "### Création des liens et des noeuds pour GEPHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ad9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liensCsv(fichier: str, tabUsers: set, unique_links: set) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère les liens entre utilisateurs et subreddits à partir d'un fichier CSV.\n",
    "    \n",
    "    :param fichier: Chemin du fichier CSV.\n",
    "    :param tabUsers: Ensemble des utilisateurs déjà traités.\n",
    "    :param unique_links: Ensemble des liens uniques déjà traités.\n",
    "    :return: DataFrame contenant les liens entre utilisateurs et subreddits.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fichier)\n",
    "    nouveau_df = pd.DataFrame(columns=['Utilisateur', 'Subreddit'])\n",
    "\n",
    "    for colonne in df.columns:\n",
    "        if len(df[colonne]) > 1 and ('.' not in colonne) and (colonne != \"AutoModerator\"):\n",
    "            subreddits = [subreddit for subreddit in df[colonne][1:] if pd.notna(subreddit)]\n",
    "            if subreddits:\n",
    "                paires = pd.DataFrame({'Utilisateur': colonne, 'Subreddit': subreddits})\n",
    "                # Ensure only unique pairs are added\n",
    "                unique_pairs = list(zip(paires['Utilisateur'], paires['Subreddit']))\n",
    "                unique_pairs = [pair for pair in unique_pairs if pair not in unique_links]\n",
    "                unique_links.update(unique_pairs)\n",
    "                paires = pd.DataFrame(unique_pairs, columns=['Utilisateur', 'Subreddit'])\n",
    "                nouveau_df = pd.concat([nouveau_df, paires], ignore_index=True)\n",
    "\n",
    "        if colonne not in tabUsers:\n",
    "            tabUsers.add(colonne)\n",
    "\n",
    "    return (nouveau_df, tabUsers, unique_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d627e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noeudsCsv(fichier: str, subList: set) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère les noeuds à partir d'un fichier CSV.\n",
    "    \n",
    "    :param fichier: Chemin du fichier CSV.\n",
    "    :param subList: Ensemble des subreddits déjà traités.\n",
    "    :return: DataFrame contenant les noeuds.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(fichier)\n",
    "    nouveau_df = pd.DataFrame(columns=['ID', 'Label', 'Type'])\n",
    "\n",
    "    for colonne in df.columns:\n",
    "        if len(df[colonne]) > 1 and ('.' not in colonne) and (colonne != \"AutoModerator\"):\n",
    "            triplet = pd.DataFrame({'ID': [colonne], 'Label': [None], 'Type': ['Utilisateur']})\n",
    "            nouveau_df = pd.concat([nouveau_df, triplet], ignore_index=True)\n",
    "\n",
    "            for ligne in df[colonne]:\n",
    "                if ligne not in subList and pd.notna(ligne):\n",
    "                    triplet = pd.DataFrame({'ID': [ligne], 'Label': [ligne], 'Type': ['SubReddit']})\n",
    "                    nouveau_df = pd.concat([nouveau_df, triplet], ignore_index=True)\n",
    "                    subList.add(ligne)\n",
    "\n",
    "    return (nouveau_df, subList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9eeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainCSVFinalOpti():\n",
    "    \"\"\"\n",
    "    Fonction principale pour créer les liens et les noeuds à partir des ficchiers CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation des listes pour stocker les noeuds et les liens\n",
    "    noeuds_list = []  \n",
    "    liens_list = []   \n",
    "    unique_links = set()  # Ensemble pour stocker les liens uniques\n",
    "\n",
    "    # Répertoire contenant les fichiers CSV\n",
    "    repertoire_csv = \"CSV_SubReddit/\"\n",
    "    fichiers_csv = [f\"CSV_SubReddit/{f}\" for f in os.listdir(repertoire_csv)]\n",
    "\n",
    "    subList = set()  # Ensemble pour stocker les subreddits uniques\n",
    "    tabUsers = set()  # Ensemble pour stocker les utilisateurs uniques\n",
    "\n",
    "    i = 1\n",
    "    # Parcours de chaque fichier CSV dans le répertoire\n",
    "    for sub in fichiers_csv:\n",
    "        print(i)\n",
    "        print(sub)\n",
    "        i += 1\n",
    "\n",
    "        # Récupération des liens à partir du fichier CSV\n",
    "        resLiens = liensCsv(sub, tabUsers, unique_links)\n",
    "        liens_list.append(resLiens[0])  \n",
    "        tabUsers.update(resLiens[1])  \n",
    "        unique_links = resLiens[2]  \n",
    "\n",
    "        # Récupération des noeuds à partir du fichier CSV\n",
    "        resNoeuds = noeudsCsv(sub, subList)\n",
    "        noeuds_list.append(resNoeuds[0])  \n",
    "        subList.update(resNoeuds[1])  \n",
    "\n",
    "    # Concaténation de tous les DataFrames de liens en un seul DataFrame\n",
    "    liens = pd.concat(liens_list, ignore_index=True) \n",
    "\n",
    "    liens = liens.drop_duplicates()\n",
    "    liens.to_csv('NSFWLiens.csv', index=False)  # Sauvegarde des liens dans un fichier CSV\n",
    "\n",
    "    # Concaténation de tous les DataFrames de noeuds en un seul DataFrame\n",
    "    noeuds = pd.concat(noeuds_list, ignore_index=True)  \n",
    "    noeuds = noeuds.drop_duplicates() \n",
    "\n",
    "    noeuds.to_csv('NSFWNoeuds.csv', index=False)  # Sauvegarde des noeuds dans un fichier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainCSVFinalOpti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4d2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyage(nbmax: int):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer les liens et les noeuds en fonction des 3000 premiers Subreddits.\n",
    "\n",
    "    : param nbmax: Nombre de Subreddits à traiter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Charger le DataFrame initial\n",
    "    df = pd.read_csv(\"OptiliensTest2.csv\")\n",
    "    dfNoeuds = pd.read_csv(\"OptinoeudsTest2.csv\")\n",
    "\n",
    "    # Charger la liste des 3000 premiers Subreddits\n",
    "    TopSubReddit = pd.read_csv(\"ultimatelist.csv\", header=None, names=[\"id\", \"Subreddit\", \"nsfw\"])\n",
    "\n",
    "    # Filtrer les lignes en fonction du Top 3000\n",
    "    df_nsfw_true = TopSubReddit[TopSubReddit['nsfw'] == 'nsfw=true']\n",
    "    df_nettoyerliens = df[df['Subreddit'].isin(df_nsfw_true['Subreddit'])]\n",
    "\n",
    "    # Récupérer les utilisateurs présents dans les liens avec \"id\" comme nom d'utilisateur\n",
    "    utilisateurs_liens = pd.DataFrame({'ID': df_nettoyerliens['Utilisateur'], 'Label': '', 'Type': 'Utilisateur'})\n",
    "\n",
    "    noeuds_subnbmax = pd.DataFrame({'ID': df_nettoyerliens['Subreddit'], 'Label': df_nettoyerliens['Subreddit'], 'Type': 'Subreddit'})\n",
    "\n",
    "    # Concaténer les nœuds initiaux avec les nouveaux utilisateurs\n",
    "    df_nettoyerNoeuds = pd.DataFrame()\n",
    "    df_nettoyerNoeuds = pd.concat([noeuds_subnbmax, utilisateurs_liens], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Supprimer les doublons pour s'assurer que les utilisateurs sont uniques dans les nœuds\n",
    "    df_nettoyerNoeuds = df_nettoyerNoeuds.drop_duplicates(subset='ID')\n",
    "\n",
    "    # Sauvegarder les nouveaux nœuds\n",
    "    df_nettoyerNoeuds.to_csv('nsfwNoeuds.csv', index=False)\n",
    "\n",
    "    # Sauvegarder le nouveau DataFrame de liens\n",
    "    df_nettoyerliens.to_csv('nsfwliens.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c352c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nettoyage(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9473007",
   "metadata": {},
   "outputs": [],
   "source": [
    "nettoyage(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
